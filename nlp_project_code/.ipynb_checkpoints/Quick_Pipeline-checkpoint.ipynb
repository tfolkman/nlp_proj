{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Quick WSD Pipeline\n",
    "\n",
    "accident-n, promise-v, behavior-n, promise-n, shirt-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.tree import ParentedTree\n",
    "from __future__ import division\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "english_parser = StanfordParser(\"/Users/tylerfolkman/stanford-parser-full-2015-01-30/stanford-parser.jar\",\n",
    "                                \"/Users/tylerfolkman/stanford-parser-full-2015-01-30/stanford-parser-3.5.1-models.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_pickle(\"../data/clean/train_df.pkl\")\n",
    "test = pd.read_pickle(\"../data/clean/test_df.pkl\")\n",
    "\n",
    "#accident train data\n",
    "accident_train_context = train[train.lextype == 'accident-n']['context'].values[:50]\n",
    "accident_train_woi = train[train.lextype == 'accident-n']['woi'].values[:50]\n",
    "accident_train_target = train[train.lextype == 'accident-n']['senseid'].values[:50]\n",
    "\n",
    "#accident test data\n",
    "accident_test_context = test[test.lextype == 'accident-n']['context'].values[:50]\n",
    "accident_test_woi = test[test.lextype == 'accident-n']['woi'].values[:50]\n",
    "accident_test_target = test[test.lextype == 'accident-n']['senseid'].values[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#functions\n",
    "def get_bigrams(values):\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), tokenizer=word_tokenize)\n",
    "    return bigram_vectorizer.fit_transform(values).toarray()\n",
    "def get_pos(pos_list, woi):\n",
    "    woi_index = find_woi_pos(pos_list, woi)\n",
    "    woi_set = []\n",
    "    for i in range(-2, 3):\n",
    "        try:\n",
    "            woi_set.append(pos_list[woi_index + i])\n",
    "        except:\n",
    "            woi_set.append(\"na\")\n",
    "    pos_set = [b for a, b in woi_set]\n",
    "    return pos_set\n",
    "def find_woi_pos(list_tuples, woi):\n",
    "    for i in range(len(list_tuples)):\n",
    "        if woi in list_tuples[i][0]:\n",
    "            return i\n",
    "def find_woi_list(listin, woi):\n",
    "    for i, leaf in enumerate(listin):\n",
    "        if woi in leaf:\n",
    "            return i\n",
    "def get_parse(tagger, woi):\n",
    "    leaves = tagger.leaves()\n",
    "    index_woi = find_woi_list(leaves, woi)\n",
    "    loc_woi = tagger.leaf_treeposition(index_woi)\n",
    "    tagger_copy = tagger\n",
    "    for i, loc in enumerate(loc_woi):\n",
    "        tagger_copy = tagger_copy[loc]\n",
    "        if i == (len(loc_woi) - 3):\n",
    "            try:\n",
    "                head_pos = tagger_copy.label()\n",
    "                head_word = tagger_copy.leaves()[0]\n",
    "            except:\n",
    "                head_pos = 'na'\n",
    "                head_word = 'na'\n",
    "        if i == (len(loc_woi) - 4):\n",
    "            try:\n",
    "                parent_head_pos = tagger_copy.label()\n",
    "                parent_head_word = tagger_copy.leaves()[0]\n",
    "            except:\n",
    "                parent_head_pos = 'na'\n",
    "                parent_head_word = 'na'\n",
    "    return [head_word, head_pos, parent_head_word, parent_head_pos]\n",
    "def get_lex_features(values, woi):\n",
    "    pos_df = pd.DataFrame(columns=['before', 'target', 'after'])\n",
    "    parse_df = pd.DataFrame(columns=['head', 'head_pos', 'parent_head', 'parent_head_pos'])\n",
    "    for i in range(len(values)):\n",
    "        tagger = ParentedTree.convert(english_parser.raw_parse(values[i])[0])\n",
    "        pos = get_pos(tagger.pos(), woi[i])\n",
    "        parse = get_parse(tagger, woi[i])\n",
    "        pos_df.loc[i] = pos\n",
    "        parse_df.loc[i] = parse\n",
    "    return pd.get_dummies(pos_df).values, pd.get_dummies(parse_df).values\n",
    "def get_features(values, woi):\n",
    "    bigrams = get_bigrams(values)\n",
    "    pos, parse = get_lex_features(values, woi)\n",
    "    return np.concatenate([bigrams, pos, parse], axis=1)\n",
    "def get_acc(y_pred, y_test):\n",
    "    num_corr = 0\n",
    "    for i, y in enumerate(y_test):\n",
    "        if y_pred[i] in y:\n",
    "            num_corr = num_corr+1\n",
    "    return num_corr / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_context = np.concatenate([accident_train_context, accident_test_context], axis=0)\n",
    "all_woi = np.concatenate([accident_train_woi, accident_test_woi], axis=0)\n",
    "#X_features = get_features(all_context, all_woi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_features[:50,:]\n",
    "y_train = accident_train_target\n",
    "X_test = X_features[50:,:]\n",
    "y_test = accident_test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "get_acc(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
