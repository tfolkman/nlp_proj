{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make Predictions\n",
    "\n",
    "* Compare to fine grain, S system, recall results\n",
    "* I think recall is defined as percentage of correct answers on all instances in test set. Which is what we are doing (basically accuracy)\n",
    "    * http://www2.denizyuret.com/ref/edmonds/senseval.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WSD:\n",
    "    \n",
    "    model_rf = RandomForestClassifier(n_estimators=10)\n",
    "    model_svm = SVC(kernel='linear', probability=True)\n",
    "    \n",
    "    def __init__(self, lex, n_iter=10):\n",
    "        self.lex = lex\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def _load_data(self, final_dir = \"/Users/tylerfolkman/GradSchool/Spring2015/NLP/project/clean/data/test_train\",\n",
    "                 sem_dir = \"../data/sem_data/\"):\n",
    "        self.train_X = np.load(\"{0}/{1}_train_X.npy\".format(final_dir, self.lex))\n",
    "        self.test_X = np.load(\"{0}/{1}_test_X.npy\".format(final_dir, self.lex))\n",
    "        self.train_target = np.load(\"{0}/{1}_train_target.npy\".format(final_dir, self.lex))\n",
    "        self.test_target = np.load(\"{0}/{1}_test_target.npy\".format(final_dir, self.lex))\n",
    "\n",
    "        self.sem_train_X = np.load(\"{0}/{1}_train_sem.npy\".format(sem_dir, self.lex))\n",
    "        self.sem_test_X = np.load(\"{0}/{1}_test_sem.npy\".format(sem_dir, self.lex))\n",
    "\n",
    "        self.comb_train_X = np.concatenate([self.train_X, self.sem_train_X], axis=1)\n",
    "        self.comb_test_X = np.concatenate([self.test_X, self.sem_test_X], axis=1)\n",
    "        \n",
    "    def _fit(self, X, y):\n",
    "        self.model_rf.fit(X, y)\n",
    "        self.model_svm.fit(X, y)\n",
    "        \n",
    "    def _predict(self, X):\n",
    "        pred_rf_prob = self.model_rf.predict_proba(X)\n",
    "        pred_svm_prob = self.model_svm.predict_proba(X)\n",
    "        \n",
    "        both_max = np.maximum(pred_rf_prob, pred_svm_prob).argmax(axis=1)\n",
    "        both_mean = np.mean( np.array([ pred_rf_prob, pred_svm_prob ]), axis=0 ).argmax(axis=1)\n",
    "        rf_max = pred_rf_prob.argmax(axis=1)\n",
    "        svm_max = pred_svm_prob.argmax(axis=1)\n",
    "        \n",
    "        self.pred_rf = self._get_classes(rf_max)\n",
    "        self.pred_svm = self._get_classes(svm_max)\n",
    "        self.pred_max = self._get_classes(both_max)\n",
    "        self.pred_mean = self._get_classes(both_mean)\n",
    "        \n",
    "    def _get_classes(self, arg_list):\n",
    "        assert np.array_equal(self.model_rf.classes_, self.model_svm.classes_)\n",
    "        pred_classes = []\n",
    "        for a in arg_list:\n",
    "            pred_classes.append(self.model_rf.classes_[a])\n",
    "        return np.array(pred_classes)\n",
    "        \n",
    "    def _get_acc(self, y_pred):\n",
    "        tp = 0\n",
    "        for i, y in enumerate(self.test_target):\n",
    "            if y_pred[i] in y:\n",
    "                tp = tp+1\n",
    "        return tp / len(y_pred)\n",
    "    \n",
    "    def _run_model(self, train_X, test_X):\n",
    "        acc_dict = defaultdict(list)\n",
    "        for i in range(self.n_iter):\n",
    "            self._fit(train_X, self.train_target)\n",
    "            self._predict(test_X)\n",
    "            rf_acc = self._get_acc(self.pred_rf)\n",
    "            svm_acc = self._get_acc(self.pred_svm)\n",
    "            max_acc = self._get_acc(self.pred_max)\n",
    "            mean_acc = self._get_acc(self.pred_mean)\n",
    "            acc_dict['randomForest'].append(rf_acc)\n",
    "            acc_dict['svm'].append(svm_acc)\n",
    "            acc_dict['both_max'].append(max_acc)\n",
    "            acc_dict['both_mean'].append(mean_acc)\n",
    "        return acc_dict\n",
    "    \n",
    "    def _run_all_models(self):\n",
    "        self._load_data()\n",
    "        model_dict = defaultdict(dict)\n",
    "        model_dict['syntax'] = self._run_model(self.train_X, self.test_X)\n",
    "        model_dict['combined'] = self._run_model(self.comb_train_X, self.comb_test_X)\n",
    "        model_dict['semantics'] = self._run_model(self.sem_train_X, self.sem_test_X)\n",
    "        self.model_dict = model_dict\n",
    "    \n",
    "    def _to_df(self):\n",
    "        self.df = pd.DataFrame.from_dict(self.model_dict, orient='index')\n",
    "    \n",
    "    def run_all_df(self):\n",
    "        self._run_all_models()\n",
    "        self._to_df()\n",
    "        return self.df\n",
    "    \n",
    "    def run_all_save(self, dir_out=\"../data/predictions/\"):\n",
    "        self._run_all_models()\n",
    "        self._to_df()\n",
    "        self.df.to_csv(dir_out + self.lex + \".csv\")\n",
    "        \n",
    "    def save_importance(self, dir_in=\"../data/sem_data_cols/\", top_n=100,\n",
    "                      dir_out = \"../data/feature_importance/\"):\n",
    "        word = self.lex.split(\"-\")[0]\n",
    "        features = np.load(dir_in + word + \".npy\")\n",
    "        self._load_data()\n",
    "        self.model_rf.fit(self.sem_train_X, self.train_target)\n",
    "        fimport = self.model_rf.feature_importances_\n",
    "        ind = np.argpartition(fimport, -top_n)[-top_n:]\n",
    "        ind = ind[np.argsort(fimport[ind])][::-1]\n",
    "        both = np.array([features[ind], fimport[ind]])\n",
    "        np.savetxt(dir_out + word + \".txt\", both, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 4412)\n",
      "(178, 5717)\n",
      "(183, 5548)\n",
      "(183, 6087)\n",
      "(1234, 9666)\n",
      "(1234, 11657)\n",
      "(1163, 3928)\n",
      "(1163, 12357)\n",
      "(307, 5882)\n",
      "(307, 7454)\n",
      "(282, 4992)\n",
      "(282, 5560)\n",
      "(259, 5751)\n",
      "(259, 7176)\n",
      "(99, 1173)\n",
      "(99, 3565)\n"
     ]
    }
   ],
   "source": [
    "excessn = WSD('excess-n')\n",
    "excessn.run_all_save()\n",
    "\n",
    "floatv = WSD('float-v')\n",
    "floatv.run_all_save()\n",
    "\n",
    "accidentn = WSD('accident-n')\n",
    "accidentn.run_all_save()\n",
    "\n",
    "promisev = WSD('promise-v')\n",
    "promisev.run_all_save()\n",
    "\n",
    "generousa = WSD('generous-a')\n",
    "generousa.run_all_save()\n",
    "\n",
    "botherv = WSD('bother-v')\n",
    "botherv.run_all_save()\n",
    "\n",
    "derivev = WSD('derive-v')\n",
    "derivev.run_all_save()\n",
    "\n",
    "sackn = WSD('sack-n')\n",
    "sackn.run_all_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441, 2690)\n",
      "(441, 9151)\n"
     ]
    }
   ],
   "source": [
    "brillianta = WSD('brilliant-a')\n",
    "brillianta.run_all_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature importance\n",
    "excessn = WSD('excess-n')\n",
    "excessn.save_importance()\n",
    "\n",
    "floatv = WSD('float-v')\n",
    "floatv.save_importance()\n",
    "\n",
    "accidentn = WSD('accident-n')\n",
    "accidentn.save_importance()\n",
    "\n",
    "promisev = WSD('promise-v')\n",
    "promisev.save_importance()\n",
    "\n",
    "generousa = WSD('generous-a')\n",
    "generousa.save_importance()\n",
    "\n",
    "botherv = WSD('bother-v')\n",
    "botherv.save_importance()\n",
    "\n",
    "derivev = WSD('derive-v')\n",
    "derivev.save_importance()\n",
    "\n",
    "sackn = WSD('sack-n')\n",
    "sackn.save_importance()\n",
    "\n",
    "brillianta = WSD('brilliant-a')\n",
    "brillianta.save_importance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
