{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data to use\n",
    "\n",
    "* excess-n because smallest n with more than 1 class and more than 100 train obs\n",
    "* float-v because smallest v with more than 1 class and more than 100 train obs\n",
    "* brillian-a because largest a\n",
    "* accident-n because largest n\n",
    "* promise-v because largest v\n",
    "\n",
    "##Assumptions:\n",
    "* Only grab POS and PARSE features for first occurance of WOI - fine in most cases, but not all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tree import ParentedTree\n",
    "from __future__ import division\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "english_parser = StanfordParser(\"/Users/tylerfolkman/stanford-parser-full-2015-01-30/stanford-parser.jar\",\n",
    "                                \"/Users/tylerfolkman/stanford-parser-full-2015-01-30/stanford-parser-3.5.1-models.jar\",\n",
    "                               java_options='-mx8000m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_pickle(\"../data/clean/train_df.pkl\")\n",
    "test = pd.read_pickle(\"../data/clean/test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accident-n     1234\n",
       "promise-v      1163\n",
       "behaviour-n     994\n",
       "promise-n       589\n",
       "shirt-n         531\n",
       "brilliant-a     441\n",
       "knee-n          417\n",
       "slight-a        380\n",
       "modest-a        374\n",
       "wooden-a        362\n",
       "giant-n         343\n",
       "giant-a         316\n",
       "generous-a      307\n",
       "seize-v         288\n",
       "bother-v        282\n",
       "bury-v          272\n",
       "promise-a       262\n",
       "derive-v        259\n",
       "calculate-v     218\n",
       "sack-v          187\n",
       "float-v         183\n",
       "amaze-a         183\n",
       "excess-n        178\n",
       "amaze-v         133\n",
       "sack-n           99\n",
       "excess-a         73\n",
       "float-n          61\n",
       "invade-v         46\n",
       "floating-a       41\n",
       "calculate-a      31\n",
       "scrap-v          30\n",
       "scrap-n          27\n",
       "onion-n          26\n",
       "knee-a           16\n",
       "bother-n         12\n",
       "consume-a        11\n",
       "invade-a          8\n",
       "slight-n          5\n",
       "seize-a           4\n",
       "brilliant-n       2\n",
       "knee-v            2\n",
       "shirt-a           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.lextype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1163, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = train[train.lextype == 'promise-v']\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accident-n     1234\n",
       "promise-v      1163\n",
       "behaviour-n     994\n",
       "promise-n       589\n",
       "shirt-n         531\n",
       "brilliant-a     441\n",
       "knee-n          417\n",
       "slight-a        380\n",
       "modest-a        374\n",
       "wooden-a        362\n",
       "giant-n         343\n",
       "giant-a         316\n",
       "generous-a      307\n",
       "seize-v         288\n",
       "bother-v        282\n",
       "bury-v          272\n",
       "promise-a       262\n",
       "derive-v        259\n",
       "calculate-v     218\n",
       "sack-v          187\n",
       "float-v         183\n",
       "amaze-a         183\n",
       "excess-n        178\n",
       "amaze-v         133\n",
       "sack-n           99\n",
       "excess-a         73\n",
       "float-n          61\n",
       "invade-v         46\n",
       "floating-a       41\n",
       "calculate-a      31\n",
       "scrap-v          30\n",
       "scrap-n          27\n",
       "onion-n          26\n",
       "knee-a           16\n",
       "bother-n         12\n",
       "consume-a        11\n",
       "invade-a          8\n",
       "slight-n          5\n",
       "seize-a           4\n",
       "brilliant-n       2\n",
       "knee-v            2\n",
       "shirt-a           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.lextype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#functions\n",
    "def find_woi_sent(context, woi):\n",
    "    for s in context:\n",
    "        if woi in s:\n",
    "            return s\n",
    "\n",
    "def get_trees(context, woi):\n",
    "    \n",
    "    assert len(context) == len(woi)\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w'-]+|[^\\w\\s]+\")\n",
    "\n",
    "    sentences = [sent_tokenize(s) for s in context]\n",
    "    single_sent = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        single_sent.append(find_woi_sent(sent, woi[i]))\n",
    "    assert len(sentences) == len(single_sent)\n",
    "\n",
    "    tokenized_context = tokenizer.tokenize_sents(single_sent)\n",
    "    tagger = ParentedTree.convert(english_parser.parse_sents(tokenized_context))\n",
    "    return tagger\n",
    "\n",
    "def get_bigrams(values):\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(2, 1), tokenizer=word_tokenize)\n",
    "    return bigram_vectorizer.fit_transform(values).toarray()\n",
    "def get_pos(pos_list, woi):\n",
    "    woi_index = find_woi_pos(pos_list, woi)\n",
    "    woi_set = []\n",
    "    for i in range(-2, 3):\n",
    "        try:\n",
    "            woi_set.append(pos_list[woi_index + i])\n",
    "        except:\n",
    "            woi_set.append(\"na\")\n",
    "    pos_set = [b for a, b in woi_set]\n",
    "    return pos_set\n",
    "def find_woi_pos(list_tuples, woi):\n",
    "    for i in range(len(list_tuples)):\n",
    "        if woi in list_tuples[i][0]:\n",
    "            return i\n",
    "def find_woi_list(listin, woi):\n",
    "    for i, leaf in enumerate(listin):\n",
    "        if woi in leaf:\n",
    "            return i\n",
    "def get_parse(tagger, woi):\n",
    "    leaves = tagger.leaves()\n",
    "    index_woi = find_woi_list(leaves, woi)\n",
    "    loc_woi = tagger.leaf_treeposition(index_woi)\n",
    "    tagger_copy = tagger\n",
    "    for i, loc in enumerate(loc_woi):\n",
    "        tagger_copy = tagger_copy[loc]\n",
    "        if i == (len(loc_woi) - 3):\n",
    "            try:\n",
    "                head_pos = tagger_copy.label()\n",
    "                head_word = tagger_copy.leaves()[0]\n",
    "            except:\n",
    "                head_pos = 'na'\n",
    "                head_word = 'na'\n",
    "        if i == (len(loc_woi) - 4):\n",
    "            try:\n",
    "                parent_head_pos = tagger_copy.label()\n",
    "                parent_head_word = tagger_copy.leaves()[0]\n",
    "            except:\n",
    "                parent_head_pos = 'na'\n",
    "                parent_head_word = 'na'\n",
    "    if len(loc_woi) < 4:\n",
    "        parent_head_pos = 'na'\n",
    "        parent_head_word = 'na'\n",
    "    return [head_word, head_pos, parent_head_word, parent_head_pos]\n",
    "def get_lex_features(values, woi):\n",
    "    pos_df = pd.DataFrame(columns=['before_before', 'before', 'target', 'after', 'after_after'])\n",
    "    parse_df = pd.DataFrame(columns=['head', 'head_pos', 'parent_head', 'parent_head_pos'])\n",
    "    trees = get_trees(values, woi)\n",
    "    n_trees = len(trees)\n",
    "    for i, tree in enumerate(trees):\n",
    "        \n",
    "        clear_output()\n",
    "        print (\"Getting POS and Parse Features: {0} / {1}\".format(i, n_trees-1))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        pos = get_pos(tree.pos(), woi[i])\n",
    "        parse = get_parse(tree, woi[i])\n",
    "        pos_df.loc[i] = pos\n",
    "        parse_df.loc[i] = parse\n",
    "        \n",
    "    return pd.get_dummies(pos_df).values, pd.get_dummies(parse_df).values\n",
    "def get_features(values, woi):\n",
    "    print(\"Getting Bigram Features...\")\n",
    "    sys.stdout.flush()\n",
    "    bigrams = get_bigrams(values)\n",
    "    print(\"Getting POS and Parse Features...\")\n",
    "    sys.stdout.flush()\n",
    "    pos, parse = get_lex_features(values, woi)\n",
    "    return bigrams, pos, parse\n",
    "def create_data_lex(lex, train=train, test=test,\n",
    "                    final_dir = \"/Users/tylerfolkman/GradSchool/Spring2015/NLP/project/clean/data/test_train\",\n",
    "                    pieces_dir = \"/Users/tylerfolkman/GradSchool/Spring2015/NLP/project/clean/data/test_train/pieces\"):\n",
    "    \n",
    "    print(\"Processing {}\".format(lex))\n",
    "    sys.stdout.flush()\n",
    "    train_set = train[train.lextype == lex]\n",
    "    test_set = test[test.lextype == lex]\n",
    "    split_point = train_set.shape[0]\n",
    "    all_context = np.concatenate([train_set['context'].values, test_set['context'].values], axis=0)\n",
    "    all_woi = np.concatenate([train_set['woi'].values, test_set['woi'].values], axis=0)\n",
    "\n",
    "    bigrams, pos, parse = get_features(all_context, all_woi)\n",
    "    print(\"Compiling Data...\")\n",
    "    sys.stdout.flush()\n",
    "    train_bigrams = bigrams[:split_point]\n",
    "    train_pos = pos[:split_point]\n",
    "    train_parse = parse[:split_point]\n",
    "    train_target = train_set['senseid'].values\n",
    "    test_bigrams = bigrams[split_point:]\n",
    "    test_pos = pos[split_point:]\n",
    "    test_parse = parse[split_point:]\n",
    "    test_target = test_set['senseid'].values\n",
    "    train_X = np.concatenate([train_bigrams, train_pos, train_parse], axis=1)\n",
    "    test_X = np.concatenate([test_bigrams, test_pos, test_parse], axis=1)\n",
    "\n",
    "    print(\"Writing to disk...\")\n",
    "    sys.stdout.flush()\n",
    "    #save final\n",
    "    np.save(\"{0}/{1}_train_X\".format(final_dir, lex), train_X)\n",
    "    np.save(\"{0}/{1}_train_target\".format(final_dir, lex), train_target)\n",
    "    np.save(\"{0}/{1}_test_X\".format(final_dir, lex), test_X)\n",
    "    np.save(\"{0}/{1}_test_target\".format(final_dir, lex), test_target)\n",
    "\n",
    "    #save pieces\n",
    "    np.save(\"{0}/{1}_train_bigrams\".format(pieces_dir, lex), train_bigrams)\n",
    "    np.save(\"{0}/{1}_train_pos\".format(pieces_dir, lex), train_pos)\n",
    "    np.save(\"{0}/{1}_train_parse\".format(pieces_dir, lex), train_parse)\n",
    "    np.save(\"{0}/{1}_test_bigrams\".format(pieces_dir, lex), test_bigrams)\n",
    "    np.save(\"{0}/{1}_test_pos\".format(pieces_dir, lex), test_pos)\n",
    "    np.save(\"{0}/{1}_test_parse\".format(pieces_dir, lex), test_parse)\n",
    "    \n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting POS and Parse Features: 363 / 363\n",
      "Compiling Data...\n",
      "Writing to disk...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "create_data_lex('excess-n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting POS and Parse Features: 411 / 411\n",
      "Compiling Data...\n",
      "Writing to disk...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "create_data_lex('float-v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting POS and Parse Features: 669 / 669\n",
      "Compiling Data...\n",
      "Writing to disk...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "create_data_lex('brilliant-a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting POS and Parse Features: 1500 / 1500\n",
      "Compiling Data...\n",
      "Writing to disk...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "create_data_lex('accident-n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting POS and Parse Features: 1386 / 1386\n",
      "Compiling Data...\n",
      "Writing to disk...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "create_data_lex('promise-v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
